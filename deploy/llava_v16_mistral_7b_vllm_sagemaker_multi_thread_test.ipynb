{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2378aa32-b90d-4b6d-896d-ef5ff93b6c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "import base64\n",
    "import json\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# 这是一个启动了Rolling Batch的vLLM的Sagemaker Endpint\n",
    "endpoint_name = 'endpoint-custome-llava-v16-2024-11-21-09-32-34' \n",
    "\n",
    "\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path, scale_size):\n",
    "    try:\n",
    "        # 打开图像文件\n",
    "        with Image.open(image_path) as img:\n",
    "            # 缩放图像\n",
    "            img = img.resize(scale_size)\n",
    "\n",
    "            # 创建BytesIO对象\n",
    "            buffer = BytesIO()\n",
    "\n",
    "            # 将缩放后的图像保存到BytesIO对象中\n",
    "            img.save(buffer, format=\"PNG\")\n",
    "\n",
    "            # 获取图像数据的字节序列\n",
    "            img_bytes = buffer.getvalue()\n",
    "\n",
    "            # 计算Base64编码\n",
    "            base64_encoded = base64.b64encode(img_bytes).decode('utf-8')\n",
    "\n",
    "            return base64_encoded\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_inference(endpoint_name, inputs):\n",
    "    response = smr_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, Body=json.dumps(inputs)\n",
    "    )\n",
    "    return response[\"Body\"].read().decode('utf-8')\n",
    "\n",
    "def call_sagemaker_llava(input_text, system_prompt, temperature, top_p, top_k, scale_size=(400, 600), input_image_paths=None, input_images=None):\n",
    "    \n",
    "    content_images = []\n",
    "    if Path(input_image_paths).is_file():\n",
    "        #print(\"file path is \", input_image_paths)\n",
    "        content_images.append(encode_image(input_image_paths, scale_size))\n",
    "    elif Path(input_image_paths).is_dir():\n",
    "        print(\"dir path is \", input_image_paths)\n",
    "        for input_image_path in Path(input_image_paths).glob('*.jpg'):\n",
    "            content_images.append(encode_image(input_image_path, scale_size))\n",
    "    \n",
    "    #print(len(content_images))\n",
    "    \n",
    "    prompt = \"# system_prompt  \\n\" + system_prompt + \"\\n===============\\n # user_input  \\n\" + input_text\n",
    "    \n",
    "    content = [{\"type\": \"text\", \"text\": prompt}]\n",
    "    \n",
    "    content += [{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}} for base64_image in content_images]\n",
    "    \n",
    "    # content += [{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}} for base64_image in [1, 2]]\n",
    "\n",
    "    # print(content)\n",
    "\n",
    "    inputs = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\":512,\n",
    "        \"temperature\": temperature, \n",
    "        \"top_p\": top_p\n",
    "      }\n",
    "    \n",
    "    t0 = time.time()\n",
    "    response = run_inference(endpoint_name, inputs)\n",
    "    t1 = time.time()\n",
    "    # print(response)\n",
    "    try:\n",
    "        outputs = json.loads(response)[\"choices\"][0][\"message\"][\"content\"]\n",
    "        usage = json.loads(response)[\"usage\"]\n",
    "        return outputs, usage, t1-t0\n",
    "    # response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    except:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23786620-28cb-4236-9ab8-7cc4b8ffc15a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' Yes, the image shows a hotel with multiple rooms, as indicated by the presence of numerous balconies and the overall layout of the building. ', {'prompt_tokens': 1675, 'completion_tokens': 30, 'total_tokens': 1705}, 1.5002124309539795) 1.536064863204956\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\n",
    "user_input = \"\"\"\n",
    "Fill in the blank: this is a photo of a {}\n",
    "\"\"\"\n",
    "image_path = \"../data0527/img/0.jpg\"\n",
    "t0 = time.time()\n",
    "r = call_sagemaker_llava(user_input, system_prompt, 0.5, 0.7, 100, scale_size=(180, 320), input_image_paths=image_path, input_images=None)\n",
    "t1 = time.time()\n",
    "print(r, t1-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501801f7-23b4-4eb9-bf99-910577a1a51f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Single Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "092b5511-4604-4528-8af2-0d1fbe50b8b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:57<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average input: 1175.0676453471767 token/s, average output: 18.235958122834656 token/s 1175.0676453471767 18.235958122834656\n",
      "average time per 1000 image 1154.8611736297607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 压力测试, 单线程1\n",
    "\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "total_input_token = 0\n",
    "total_output_token = 0\n",
    "\n",
    "t0 = time.time()\n",
    "for i in tqdm(range(50)):\n",
    "    h, l = random.randint(200, 400), random.randint(200, 400)\n",
    "    _, r, _ = call_sagemaker_llava(user_input, system_prompt, 0.5, 0.7, 100, scale_size=(h, l), input_image_paths=image_path, input_images=None)\n",
    "    total_input_token += r[\"prompt_tokens\"]\n",
    "    total_output_token += r[\"completion_tokens\"]\n",
    "t1 = time.time()\n",
    "dt = t1-t0\n",
    "\n",
    "it = total_input_token/dt\n",
    "ot = total_output_token/dt\n",
    "\n",
    "print(f\"average input: {it} token/s, average output: {ot} token/s\", it, ot)\n",
    "print (f\"average time per 1000 image\",dt/50*1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2df82a7f-0397-453d-98df-5519a10b7afb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38663222829620314 0.11475746786428097 0.5013896961604841\n"
     ]
    }
   ],
   "source": [
    "# price calculate\n",
    "\n",
    "instance_price = 1.515 # usd/h\n",
    "\n",
    "claude_3_Haiku_price_input_token, claude_3_Haiku_price_output_token = 0.00025, 0.00125 # usd/1k token\n",
    "\n",
    "price_input_token_1h = it * 3600 / 1000 * claude_3_Haiku_price_input_token\n",
    "price_output_token_1h = ot * 3600 / 1000 * claude_3_Haiku_price_output_token\n",
    "Haiku_price_total = price_input_token_1h + price_output_token_1h\n",
    "\n",
    "print(price_input_token_1h, price_output_token_1h, Haiku_price_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0178516a-657e-4820-b102-041a7692c68f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:32<00:00,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average input: 676.1326171011988 token/s, average output: 23.358456426465317 token/s 676.1326171011988 23.358456426465317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 压力测试, 单线程2\n",
    "\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "total_input_token = 0\n",
    "total_output_token = 0\n",
    "total_t = 0\n",
    "\n",
    "t0 = time.time()\n",
    "for i in tqdm(range(10)):\n",
    "    # h, l = random.randint(400, 600), random.randint(600, 800)\n",
    "    h, l = 360, 640\n",
    "    _, r, t = call_sagemaker_llava(user_input, system_prompt, 0.1, 0.1, 100, scale_size=(h, l), input_image_paths=image_path, input_images=None)\n",
    "    total_input_token += r[\"prompt_tokens\"]\n",
    "    total_output_token += r[\"completion_tokens\"]\n",
    "    total_t += t\n",
    "t1 = time.time()\n",
    "\n",
    "it = total_input_token/total_t\n",
    "ot = total_output_token/total_t\n",
    "\n",
    "print(f\"average input: {it} token/s, average output: {ot} token/s\", it, ot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "701c9616-3f3a-4656-97bb-f30dd566dc16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6085193553910789 0.10511305391909392 0.7136324093101729\n"
     ]
    }
   ],
   "source": [
    "# price calculate\n",
    "\n",
    "instance_price = 1.515 # usd/h\n",
    "\n",
    "claude_3_Haiku_price_input_token, claude_3_Haiku_price_output_token = 0.00025, 0.00125 # usd/1k token\n",
    "\n",
    "price_input_token_1h = it * 3600 / 1000 * claude_3_Haiku_price_input_token\n",
    "price_output_token_1h = ot * 3600 / 1000 * claude_3_Haiku_price_output_token\n",
    "Haiku_price_total = price_input_token_1h + price_output_token_1h\n",
    "\n",
    "print(price_input_token_1h, price_output_token_1h, Haiku_price_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3a4a27b-da4f-4624-b6c9-66f4f17df118",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6085193553910789 0.10511305391909392 0.7136324093101729\n"
     ]
    }
   ],
   "source": [
    "# price calculate\n",
    "\n",
    "instance_price = 1.19 # usd/h\n",
    "\n",
    "claude_3_Haiku_price_input_token, claude_3_Haiku_price_output_token = 0.00025, 0.00125 # usd/1k token\n",
    "\n",
    "price_input_token_1h = it * 3600 / 1000 * claude_3_Haiku_price_input_token\n",
    "price_output_token_1h = ot * 3600 / 1000 * claude_3_Haiku_price_output_token\n",
    "Haiku_price_total = price_input_token_1h + price_output_token_1h\n",
    "\n",
    "print(price_input_token_1h, price_output_token_1h, Haiku_price_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0a8819-ddf6-4286-a193-e01467d7b93b",
   "metadata": {},
   "source": [
    "# Multi-Thread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b452edad-0709-42d9-a801-8d43de8804cf",
   "metadata": {},
   "source": [
    "修改max_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "321d0d4c-8364-4264-9ff1-78a9ffc3894d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:36<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average input: 2957.0911276148563 token/s, average output: 30.092987120317776 token/s 2957.0911276148563 30.092987120317776\n",
      "average time per 1000 image 747.0178985595703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 压力测试, 多线程\n",
    "\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "total_input_token = 0\n",
    "total_output_token = 0\n",
    "max_workers=64\n",
    "\n",
    "def test_function(i):\n",
    "    global total_input_token, total_output_token\n",
    "    h, l = random.randint(200, 400), random.randint(200, 400)\n",
    "    _, r, _ = call_sagemaker_llava(user_input, system_prompt, 0.5, 0.7, 100, input_image_paths=image_path, input_images=None)\n",
    "    total_input_token += r[\"prompt_tokens\"]\n",
    "    total_output_token += r[\"completion_tokens\"]\n",
    "\n",
    "t0 = time.time()\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [executor.submit(test_function, i) for i in range(50)]\n",
    "    for future in tqdm(futures):\n",
    "        future.result()\n",
    "t1 = time.time()\n",
    "dt = t1-t0\n",
    "\n",
    "it = total_input_token/dt\n",
    "ot = total_output_token/dt\n",
    "\n",
    "print(f\"average input: {it} token/s, average output: {ot} token/s\", it, ot)\n",
    "print (f\"average time per 1000 image\",dt/50*1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d9cafff-119a-496e-9f9a-61eb76123600",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.812429666519165"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ccdaafd3-dc5d-47f9-b486-af26bf6e3c32",
   "metadata": {},
   "source": [
    "现有Llava-v16-Mistral-vLLM-7B Sagemaker endpint on g5.xlarge的测试结果：\n",
    "batch=64, total=747, avg=0.74s\n",
    "batch=32, total=700s, avg=0.7s\n",
    "batch=16, total=725, avg=0.72\n",
    "batch=8, total=806s, avg=0.81s\n",
    "\n",
    "\n",
    "#single, total 1154, avg = 1.15s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
